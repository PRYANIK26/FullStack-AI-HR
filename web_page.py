import streamlit as st
import os
from dotenv import load_dotenv
from openai import OpenAI
import json
import docx
import PyPDF2
from striprtf.striprtf import rtf_to_text
import plotly.graph_objects as go
import plotly.express as px
import sqlite3
import uuid
from datetime import datetime
import pandas as pd
import re
import requests


MAIN_SERVER_URL = f"http://localhost:{os.getenv('MAIN_SERVER_PORT', 5000)}/approve-interview"

load_dotenv()

st.set_page_config(
    page_title="HR-–ê–≤–∞—Ç–∞—Ä: –ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤",
    page_icon="ü§ñ",
    layout="wide"
)

if 'current_view' not in st.session_state:
    st.session_state.current_view = 'main'
if 'selected_analysis_id' not in st.session_state:
    st.session_state.selected_analysis_id = None
if 'analysis_mode' not in st.session_state:
    st.session_state.analysis_mode = 'single'

class DatabaseManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è SQLite/PostgreSQL"""
    
    def __init__(self, db_type='sqlite'):
        self.db_type = db_type
        if db_type == 'sqlite':
            self.db_path = 'hr_analysis.db'
            self._init_sqlite()
        elif db_type == 'postgresql':
            self._init_postgresql()
    
    def _init_sqlite(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite"""
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self._create_tables()
    
    def _init_postgresql(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PostgreSQL (–¥–ª—è Railway)"""
        import psycopg2
        from psycopg2.extras import RealDictCursor
        
        DATABASE_URL = os.getenv('DATABASE_URL')
        self.conn = psycopg2.connect(DATABASE_URL, cursor_factory=RealDictCursor)
        self._create_tables()
    
    def _create_tables(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        cursor = self.conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS vacancies (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                industry TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS candidates (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                resume_content TEXT NOT NULL,
                email TEXT,
                phone TEXT,
                telegram TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analyses (
                id TEXT PRIMARY KEY,
                vacancy_id TEXT NOT NULL,
                candidate_id TEXT NOT NULL,
                result_json TEXT NOT NULL,
                total_score INTEGER,
                recommendation TEXT,
                confidence_level TEXT,
                analysis_type TEXT DEFAULT 'single',
                needs_interview BOOLEAN DEFAULT FALSE,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (vacancy_id) REFERENCES vacancies (id),
                FOREIGN KEY (candidate_id) REFERENCES candidates (id)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS interviews (
                id TEXT PRIMARY KEY,
                analysis_id TEXT NOT NULL,
                candidate_name TEXT NOT NULL,
                vacancy_title TEXT NOT NULL,
                start_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                end_time TIMESTAMP,
                duration_seconds INTEGER,
                total_questions INTEGER DEFAULT 0,
                interview_plan TEXT,  -- JSON
                final_scores TEXT,    -- JSON
                final_recommendation TEXT,
                interviewer_notes TEXT,
                phase_breakdown TEXT, -- JSON
                adaptive_insights TEXT, -- JSON
                repetition_analysis TEXT, -- JSON - –ù–û–í–û–ï –ü–û–õ–ï
                timing_statistics TEXT, -- JSON - –ù–û–í–û–ï –ü–û–õ–ï  
                advanced_analytics TEXT, -- JSON - –ù–û–í–û–ï –ü–û–õ–ï
                status TEXT DEFAULT 'active',
                FOREIGN KEY (analysis_id) REFERENCES analyses (id)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS interview_qa (
                id TEXT PRIMARY KEY,
                interview_id TEXT NOT NULL,
                question_number INTEGER,
                question_area TEXT,
                question_text TEXT NOT NULL,
                question_difficulty TEXT,
                question_phase TEXT,
                answer_text TEXT,
                answer_duration_seconds INTEGER,
                technical_score INTEGER,
                communication_score INTEGER,
                depth_score INTEGER,
                confidence_score INTEGER,
                practical_experience INTEGER,
                red_flags TEXT,  -- JSON
                strengths_shown TEXT,  -- JSON
                analysis_notes TEXT,
                knowledge_gaps TEXT,  -- JSON - –ù–û–í–û–ï –ü–û–õ–ï
                adaptation_needed TEXT,  -- –ù–û–í–û–ï –ü–û–õ–ï
                repetition_detected BOOLEAN DEFAULT FALSE,  -- –ù–û–í–û–ï –ü–û–õ–ï
                alternative_strategy_used TEXT,  -- –ù–û–í–û–ï –ü–û–õ–ï
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (interview_id) REFERENCES interviews (id)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS interview_timing (
                id TEXT PRIMARY KEY,
                interview_id TEXT NOT NULL,
                question_number INTEGER,
                question_start_time TIMESTAMP,
                answer_duration_seconds INTEGER,
                analysis_duration_seconds INTEGER,
                phase TEXT,
                time_status TEXT,  -- on_track, need_acceleration, critical_time
                remaining_minutes INTEGER,
                FOREIGN KEY (interview_id) REFERENCES interviews (id)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS final_evaluations (
                id TEXT PRIMARY KEY,
                analysis_id TEXT NOT NULL UNIQUE,
                evaluation_summary TEXT,
                final_recommendation TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (analysis_id) REFERENCES analyses (id)
            )
        ''')

        try: cursor.execute('ALTER TABLE candidates ADD COLUMN email TEXT')
        except: pass
        try: cursor.execute('ALTER TABLE candidates ADD COLUMN phone TEXT')
        except: pass
        try: cursor.execute('ALTER TABLE candidates ADD COLUMN telegram TEXT')
        except: pass
        try: cursor.execute('ALTER TABLE analyses ADD COLUMN needs_interview BOOLEAN DEFAULT FALSE')
        except: pass
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS batch_analyses (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                analysis_type TEXT NOT NULL,
                total_analyses INTEGER DEFAULT 0,
                completed_analyses INTEGER DEFAULT 0,
                results_json TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        self.conn.commit()
    
    def save_vacancy(self, title, content, industry=None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏"""
        vacancy_id = str(uuid.uuid4())
        cursor = self.conn.cursor()
        cursor.execute(
            "INSERT INTO vacancies (id, title, content, industry) VALUES (?, ?, ?, ?)",
            (vacancy_id, title, content, industry)
        )
        self.conn.commit()
        return vacancy_id
    
    def save_candidate(self, name, resume_content, email=None, phone=None, telegram=None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        candidate_id = str(uuid.uuid4())
        cursor = self.conn.cursor()
        cursor.execute(
            "INSERT INTO candidates (id, name, resume_content, email, phone, telegram) VALUES (?, ?, ?, ?, ?, ?)",
            (candidate_id, name, resume_content, email, phone, telegram)
        )
        self.conn.commit()
        return candidate_id
    
    def update_candidate_contacts(self, candidate_id, email=None, phone=None, telegram=None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–∞"""
        cursor = self.conn.cursor()
        cursor.execute(
            "UPDATE candidates SET email = ?, phone = ?, telegram = ? WHERE id = ?",
            (email, phone, telegram, candidate_id)
        )
        self.conn.commit()
    
    def save_analysis(self, vacancy_id, candidate_id, result_json, total_score, recommendation, confidence_level, analysis_type='single', needs_interview=False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        analysis_id = str(uuid.uuid4())
        cursor = self.conn.cursor()
        cursor.execute(
            "INSERT INTO analyses (id, vacancy_id, candidate_id, result_json, total_score, recommendation, confidence_level, analysis_type, needs_interview) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (analysis_id, vacancy_id, candidate_id, json.dumps(result_json), total_score, recommendation, confidence_level, analysis_type, needs_interview)
        )
        self.conn.commit()
        return analysis_id
    
    def update_interview_status(self, analysis_id, needs_interview):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è"""
        cursor = self.conn.cursor()
        cursor.execute(
            "UPDATE analyses SET needs_interview = ? WHERE id = ?",
            (needs_interview, analysis_id)
        )
        self.conn.commit()
    
    def save_batch_analysis(self, name, analysis_type, results):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ batch –∞–Ω–∞–ª–∏–∑–∞"""
        batch_id = str(uuid.uuid4())
        cursor = self.conn.cursor()
        cursor.execute(
            "INSERT INTO batch_analyses (id, name, analysis_type, total_analyses, completed_analyses, results_json) VALUES (?, ?, ?, ?, ?, ?)",
            (batch_id, name, analysis_type, len(results), len(results), json.dumps(results))
        )
        self.conn.commit()
        return batch_id
    
    def get_all_analyses(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–∞–ª–∏—á–∏–∏ –∏–Ω—Ç–µ—Ä–≤—å—é"""
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT 
                a.id, 
                a.total_score,
                a.recommendation,
                a.created_at,
                v.title as vacancy_title, 
                c.name as candidate_name,
                MAX(i.id) as interview_id, -- –ò—Å–ø–æ–ª—å–∑—É–µ–º MAX —á—Ç–æ–±—ã —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–Ω—Ç–µ—Ä–≤—å—é
                fe.id as final_evaluation_id
            FROM analyses a
            JOIN vacancies v ON a.vacancy_id = v.id
            JOIN candidates c ON a.candidate_id = c.id
            LEFT JOIN interviews i ON a.id = i.analysis_id AND i.status = 'completed'
            LEFT JOIN final_evaluations fe ON a.id = fe.analysis_id
            GROUP BY a.id -- –ì–†–£–ü–ü–ò–†–£–ï–ú –ø–æ ID –∞–Ω–∞–ª–∏–∑–∞, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
            ORDER BY a.created_at DESC
        ''')
        return cursor.fetchall()
    
    def get_analysis_by_id(self, analysis_id):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ ID"""
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT a.*, v.title as vacancy_title, v.content as vacancy_content,
                   c.name as candidate_name, c.resume_content as candidate_resume,
                   c.email as candidate_email, c.phone as candidate_phone, c.telegram as candidate_telegram
            FROM analyses a
            JOIN vacancies v ON a.vacancy_id = v.id
            JOIN candidates c ON a.candidate_id = c.id
            WHERE a.id = ?
        ''', (analysis_id,))
        return cursor.fetchone()

    def get_interview_by_analysis_id(self, analysis_id):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –ø–æ –∏–Ω—Ç–µ—Ä–≤—å—é –ø–æ ID –∞–Ω–∞–ª–∏–∑–∞"""
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT * FROM interviews WHERE analysis_id = ? AND status = 'completed'
        ''', (analysis_id,))
        interview_data = cursor.fetchone()

        if not interview_data:
            return None, None
        
        cursor.execute('''
            SELECT * FROM interview_qa WHERE interview_id = ? ORDER BY question_number ASC
        ''', (interview_data['id'],))
        qa_data = cursor.fetchall()
        
        return dict(interview_data), [dict(row) for row in qa_data]

    def search_analyses(self, search_term):
        """–ü–æ–∏—Å–∫ –∞–Ω–∞–ª–∏–∑–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–∞–ª–∏—á–∏–∏ –∏–Ω—Ç–µ—Ä–≤—å—é"""
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT 
                a.*, 
                v.title as vacancy_title, 
                c.name as candidate_name,
                i.id as interview_id,
                fe.id as final_evaluation_id
            FROM analyses a
            JOIN vacancies v ON a.vacancy_id = v.id
            JOIN candidates c ON a.candidate_id = c.id
            LEFT JOIN interviews i ON a.id = i.analysis_id AND i.status = 'completed'
            LEFT JOIN final_evaluations fe ON a.id = fe.analysis_id
            WHERE v.title LIKE ? OR c.name LIKE ? OR a.recommendation LIKE ?
            ORDER BY a.created_at DESC
        ''', (f'%{search_term}%', f'%{search_term}%', f'%{search_term}%'))
        return cursor.fetchall()

    def save_final_evaluation(self, analysis_id, summary, recommendation):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è"""
        eval_id = str(uuid.uuid4())
        cursor = self.conn.cursor()
        cursor.execute(
            "INSERT OR REPLACE INTO final_evaluations (id, analysis_id, evaluation_summary, final_recommendation) VALUES (?, ?, ?, ?)",
            (eval_id, analysis_id, summary, recommendation)
        )
        self.conn.commit()

    def get_final_evaluation(self, analysis_id):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT * FROM final_evaluations WHERE analysis_id = ?", (analysis_id,))
        return cursor.fetchone()

class CandidateEvaluator:
    def __init__(self):
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            st.error("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ!")
            st.stop()
            
        try:
            self.client = OpenAI(api_key=api_key)
        except TypeError:
            import openai
            openai.api_key = api_key
            self.client = None
    
    def classify_industry(self, vacancy_text):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç—Ä–∞—Å–ª–∏ —á–µ—Ä–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã–π API –≤—ã–∑–æ–≤"""
        classification_prompt = f"""
–û–ø—Ä–µ–¥–µ–ª–∏ –æ—Ç—Ä–∞—Å–ª—å –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ —Å—Ç—Ä–æ–≥–æ —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞:
- tech (IT, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, DevOps, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ü–û)
- fintech (—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –±–∞–Ω–∫–∏, –ø–ª–∞—Ç–µ–∂–∏, —Ñ–∏–Ω—Ç–µ—Ö, –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã)
- healthcare (–º–µ–¥–∏—Ü–∏–Ω–∞, –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ, —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞, –±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏)
- retail (—Ä–æ–∑–Ω–∏—á–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è, e-commerce, –ø—Ä–æ–¥–∞–∂–∏, –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤)
- manufacturing (–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ, –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å, –∑–∞–≤–æ–¥—ã, –ª–æ–≥–∏—Å—Ç–∏–∫–∞)
- consulting (–∫–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥, —Å—Ç—Ä–∞—Ç–µ–≥–∏—è, –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑, —É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ —É—Å–ª—É–≥–∏)
- education (–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –æ–±—É—á–µ–Ω–∏–µ, –∞–∫–∞–¥–µ–º–∏—è, —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã, —à–∫–æ–ª—ã)
- government (–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ —É—Å–ª—É–≥–∏, –º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã, –≥–æ—Å—Å—Ç—Ä—É–∫—Ç—É—Ä—ã)

–í–ê–ñ–ù–û: 
- –£—á–∏—Ç—ã–≤–∞–π –û–°–ù–û–í–ù–£–Æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ–∑–∏—Ü–∏–∏, –∞ –Ω–µ –≤—Ç–æ—Ä–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
- –ï—Å–ª–∏ –≤ IT-–≤–∞–∫–∞–Ω—Å–∏–∏ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è "–æ–±—É—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã" - —ç—Ç–æ –≤—Å–µ —Ä–∞–≤–Ω–æ tech, –∞ –Ω–µ education
- –ï—Å–ª–∏ –≤ —Ñ–∏–Ω—Ç–µ—Ö-–≤–∞–∫–∞–Ω—Å–∏–∏ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞" - —ç—Ç–æ fintech, –∞ –Ω–µ tech

–¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏:
{vacancy_text}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç—Ä–∞—Å–ª–∏ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º:"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-5-mini",
                messages=[
                    {"role": "user", "content": classification_prompt}
                ]
            )
            industry = response.choices[0].message.content.strip().lower()
            
            valid_industries = ['tech', 'fintech', 'healthcare', 'retail', 'manufacturing', 
                              'consulting', 'education', 'government']
            
            if industry in valid_industries:
                return industry
            else:
                return 'tech'
                
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Ç—Ä–∞—Å–ª–∏: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑.")
            return 'tech'
    
    def extract_text_from_file(self, uploaded_file):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        try:
            uploaded_file.seek(0)
            file_name = uploaded_file.name.lower()
            
            if file_name.endswith('.docx'):
                doc = docx.Document(uploaded_file)
                text_parts = []
                
                for paragraph in doc.paragraphs:
                    text = paragraph.text.strip()
                    if text:
                        text_parts.append(text)
                
                for table in doc.tables:
                    for row in table.rows:
                        row_data = []
                        for cell in row.cells:
                            cell_text = cell.text.strip()
                            if cell_text:
                                row_data.append(cell_text)
                        if row_data:
                            text_parts.append(" | ".join(row_data))
                
                result = "\n".join(text_parts)
                return result if result else "–§–∞–π–ª –ø—É—Å—Ç–æ–π"
                
            elif file_name.endswith('.pdf'):
                uploaded_file.seek(0)
                pdf_reader = PyPDF2.PdfReader(uploaded_file)
                text_parts = []
                
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        text = page.extract_text().strip()
                        if text:
                            text_parts.append(text)
                    except Exception:
                        continue
                
                return "\n\n".join(text_parts) if text_parts else "PDF –ø—É—Å—Ç–æ–π"
                
            elif file_name.endswith('.rtf'):
                uploaded_file.seek(0)
                rtf_content = uploaded_file.read().decode('utf-8', errors='ignore')
                try:
                    text = rtf_to_text(rtf_content)
                    return text if text.strip() else "RTF —Ñ–∞–π–ª –ø—É—Å—Ç–æ–π"
                except Exception as e:
                    return f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RTF: {e}"
                    
            elif file_name.endswith('.txt'):
                uploaded_file.seek(0)
                text = uploaded_file.read().decode('utf-8', errors='ignore')
                return text if text.strip() else "TXT —Ñ–∞–π–ª –ø—É—Å—Ç–æ–π"
                
            else:
                uploaded_file.seek(0)
                text = uploaded_file.read().decode('utf-8', errors='ignore')
                return text if text.strip() else "–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"
                
        except Exception as e:
            return f"–û–®–ò–ë–ö–ê: {str(e)}"
    
    
    def create_evaluation_prompt(self, resume_text, vacancy_text, industry):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ—Ç—Ä–∞—Å–ª—å—é"""
        
        industry_context = {
            'tech': {
                'critical_factors': ['–∞–∫—Ç—É–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫', '–æ–ø—ã—Ç —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏', '–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏', '–∑–Ω–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö'],
                'red_flags': ['—É—Å—Ç–∞—Ä–µ–≤—à–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –±–µ–∑ —Ä–∞–∑–≤–∏—Ç–∏—è', '–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞', '—Å–ª–∞–±—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏', '–Ω–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤'],
                'bonus_points': ['contribution –≤ open source', '–ª–∏–¥–µ—Ä—Å—Ç–≤–æ –≤ —Ç–µ—Ö–∫–æ–º–∞–Ω–¥–µ', '–æ–ø—ã—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π', '–º–µ–Ω—Ç–æ—Ä—Å—Ç–≤–æ junior —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤', '—É—á–∞—Å—Ç–∏–µ –≤ tech-–∫–æ–º—å—é–Ω–∏—Ç–∏']
            },
            'fintech': {
                'critical_factors': ['–æ–ø—ã—Ç –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —É—Å–ª—É–≥–∞—Ö', '–∑–Ω–∞–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π', '–ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∏—Å–∫–æ–≤', '–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –ø–ª–∞—Ç–µ–∂–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏'],
                'red_flags': ['–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –æ–ø—ã—Ç–∞', '–Ω–µ–∑–Ω–∞–Ω–∏–µ compliance —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π', '–Ω–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∏—Å–∫–æ–≤', '—Ç–æ–ª—å–∫–æ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è'],
                'bonus_points': ['—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã FRM/CFA/PRM', '–æ–ø—ã—Ç –≤ —Ç–æ–ø –±–∞–Ω–∫–∞—Ö', '–∑–Ω–∞–Ω–∏–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤', '–æ–ø—ã—Ç –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Ñ–∏–Ω—Ç–µ—Ö —Ä–µ—à–µ–Ω–∏–π']
            },
            'healthcare': {
                'critical_factors': ['–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ/–ª–∏—Ü–µ–Ω–∑–∏–∏', '–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –ø–∞—Ü–∏–µ–Ω—Ç–∞–º–∏', '–∑–Ω–∞–Ω–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤', '–ø–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–∏–∫–∏ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è'],
                'red_flags': ['–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è', '–Ω–µ—Ç –æ–ø—ã—Ç–∞ —Å –ø–∞—Ü–∏–µ–Ω—Ç–∞–º–∏', '–Ω–µ–∑–Ω–∞–Ω–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤', '–Ω–∞—Ä—É—à–µ–Ω–∏—è —ç—Ç–∏–∫–∏'],
                'bonus_points': ['—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã', '–Ω–∞—É—á–Ω—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏', '–æ–ø—ã—Ç –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –æ–ø—ã—Ç']
            },
            'retail': {
                'critical_factors': ['–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏', '–ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è', '–Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–¥–∞–∂', '–∑–Ω–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π'],
                'red_flags': ['–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞', '–Ω–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–æ–∑–Ω–∏—á–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤', '—Å–ª–∞–±—ã–µ –∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–≤—ã–∫–∏', '–Ω–µ–∑–Ω–∞–Ω–∏–µ digital retail'],
                'bonus_points': ['–æ–ø—ã—Ç –∑–∞–ø—É—Å–∫–∞ –Ω–æ–≤—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤', '–∑–Ω–∞–Ω–∏–µ omnichannel', '–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏', '–æ–ø—ã—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏']
            },
            'manufacturing': {
                'critical_factors': ['–æ–ø—ã—Ç –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤', '–∑–Ω–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏', '–ø–æ–Ω–∏–º–∞–Ω–∏–µ lean/six sigma', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ'],
                'red_flags': ['–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–ø—ã—Ç–∞', '–Ω–µ–∑–Ω–∞–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞', '–Ω–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏', '—Ç–æ–ª—å–∫–æ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è'],
                'bonus_points': ['—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã lean/six sigma', '–æ–ø—ã—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏', '–∑–Ω–∞–Ω–∏–µ IoT/Industry 4.0', '–æ–ø—ã—Ç –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞']
            },
            'consulting': {
                'critical_factors': ['–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏', '–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ', '–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏'],
                'red_flags': ['—Å–ª–∞–±—ã–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏', '–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞', '–Ω–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è', '–ø–ª–æ—Ö–∏–µ –∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–≤—ã–∫–∏'],
                'bonus_points': ['MBA —Ç–æ–ø —à–∫–æ–ª', '–æ–ø—ã—Ç –≤ —Ç–æ–ø –∫–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥–µ', '–æ—Ç—Ä–∞—Å–ª–µ–≤–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π –æ–ø—ã—Ç', '–æ–ø—ã—Ç —Ü–∏—Ñ—Ä–æ–≤–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏']
            },
            'education': {
                'critical_factors': ['–ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–æ–ø—ã—Ç –ø—Ä–µ–ø–æ–¥–∞–≤–∞–Ω–∏—è', '–∑–Ω–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∏–∫ –æ–±—É—á–µ–Ω–∏—è', '—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º'],
                'red_flags': ['–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞', '–Ω–µ–∑–Ω–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–∏–∫', '—Å–ª–∞–±—ã–µ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏', '–Ω–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è'],
                'bonus_points': ['—É—á–µ–Ω–∞—è —Å—Ç–µ–ø–µ–Ω—å', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã', '–æ–ø—ã—Ç e-learning', '–Ω–∞—É—á–Ω—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏', '–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥–∏–∫–∏']
            },
            'government': {
                'critical_factors': ['–∑–Ω–∞–Ω–∏–µ –≥–æ—Å—Å–ª—É–∂–±—ã', '–ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤', '–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç–æ–º', '–∑–Ω–∞–Ω–∏–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞'],
                'red_flags': ['–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≥–æ—Å–æ–ø—ã—Ç–∞', '–Ω–µ–∑–Ω–∞–Ω–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑—ã', '–Ω–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –±—é—Ä–æ–∫—Ä–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤', '–ø—Ä–æ–±–ª–µ–º—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç–æ–º'],
                'bonus_points': ['—é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–æ–ø—ã—Ç –≤ —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∞—Ö', '–∑–Ω–∞–Ω–∏–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø—Ä–∞–≤–∞', '–æ–ø—ã—Ç —Ü–∏—Ñ—Ä–æ–≤–∏–∑–∞—Ü–∏–∏ –≥–æ—Å—É—Å–ª—É–≥']
            }
        }
        
        context = industry_context.get(industry, {
            'critical_factors': ['—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –æ–ø—ã—Ç', '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —É—Ä–æ–≤–Ω—è –ø–æ–∑–∏—Ü–∏–∏', '–∫—É–ª—å—Ç—É—Ä–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ'],
            'red_flags': ['—á–∞—Å—Ç–∞—è —Å–º–µ–Ω–∞ —Ä–∞–±–æ—Ç—ã', '–Ω–µ—è—Å–Ω–∞—è –º–æ—Ç–∏–≤–∞—Ü–∏—è', '–∑–∞–≤—ã—à–µ–Ω–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è'],
            'bonus_points': ['—Å—Ç–∞–±–∏–ª—å–Ω–∞—è –∫–∞—Ä—å–µ—Ä–∞', '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–ª–∏–¥–µ—Ä—Å–∫–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞']
        })
        
        return f"""
–¢—ã - –≤–µ–¥—É—â–∏–π HR-—ç–∫—Å–ø–µ—Ä—Ç —Å 20+ –≥–æ–¥–∞–º–∏ –æ–ø—ã—Ç–∞ –≤ –ø–æ–¥–±–æ—Ä–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∞ –≤ –æ—Ç—Ä–∞—Å–ª–∏ {industry.upper()}.

=== –ö–û–ù–¢–ï–ö–°–¢ –û–¢–†–ê–°–õ–ò: {industry.upper()} ===
–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –§–ê–ö–¢–û–†–´: {', '.join(context['critical_factors'])}
–ö–†–ê–°–ù–´–ï –§–õ–ê–ì–ò: {', '.join(context['red_flags'])}
–ë–û–ù–£–°–ù–´–ï –ë–ê–õ–õ–´: {', '.join(context['bonus_points'])}

=== –í–ê–ö–ê–ù–°–ò–Ø ===
{vacancy_text}

=== –†–ï–ó–Æ–ú–ï –ö–ê–ù–î–ò–î–ê–¢–ê ===  
{resume_text}

=== –≠–ö–°–ü–ï–†–¢–ù–´–ô –ê–ù–ê–õ–ò–ó ===

–ü—Ä–æ–≤–µ–¥–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏:

**–≠–¢–ê–ü 1: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–û–ù–¢–ê–ö–¢–ù–û–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò**
–í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–µ–∑—é–º–µ –∏ –Ω–∞–π–¥–∏:
- Email –∞–¥—Ä–µ—Å –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
- –ù–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ (–≤ –ª—é–±–æ–º —Ñ–æ—Ä–º–∞—Ç–µ)
- Telegram (username, —Å—Å—ã–ª–∫–∏ t.me, telegram.me, –∏–ª–∏ @username)

**–≠–¢–ê–ü 2: –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó –í–ê–ö–ê–ù–°–ò–ò**
- –ö–ª—é—á–µ–≤—ã–µ must-have —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (–Ω–∞—Ä—É—à–µ–Ω–∏–µ = –∞–≤—Ç–æ–æ—Ç–∫–∞–∑)
- Nice-to-have —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (–≤–ª–∏—è—é—Ç –Ω–∞ –æ–±—â–∏–π –±–∞–ª–ª)
- –°–∫—Ä—ã—Ç—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (—á–∏—Ç–∞–π –º–µ–∂–¥—É —Å—Ç—Ä–æ–∫)
- –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á (junior/middle/senior/expert)
- –ö–æ–º–∞–Ω–¥–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ (–ª–∏–¥–µ—Ä—Å—Ç–≤–æ/–∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ)

**–≠–¢–ê–ü 3: –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï –ö–ê–ù–î–ò–î–ê–¢–ê**  
- –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è vs –∑–∞—è–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏
- –ì–ª—É–±–∏–Ω–∞ —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã (–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–µ/—Å—Ä–µ–¥–Ω–µ–µ/—ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ)
- –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è —Ä–∞–∑–≤–∏—Ç–∏—è (—Ä–æ—Å—Ç/—Å—Ç–∞–≥–Ω–∞—Ü–∏—è/–¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è)
- –ú–æ—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
- –ö—É–ª—å—Ç—É—Ä–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–º–∞–Ω–¥–µ

**–≠–¢–ê–ü 4: –ú–ù–û–ì–û–£–†–û–í–ù–ï–í–û–ï –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–ï**

**–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø (–í–ï–°: 40%)**
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏ (–≤–µ—Ç–æ-—Ñ–∞–∫—Ç–æ—Ä)
- –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–ø—ã—Ç 
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ/—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
[–ï—Å–ª–∏ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã - –º–∞–∫—Å–∏–º—É–º 30 –±–∞–ª–ª–æ–≤ –æ–±—â–∏–π]

**–ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–ê–Ø –≠–ö–°–ü–ï–†–¢–ò–ó–ê (–í–ï–°: 35%)**  
- –ì–ª—É–±–∏–Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π
- –ö–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤
- –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏
- –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥

**–û–¢–†–ê–°–õ–ï–í–ê–Ø –°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø (–í–ï–°: 20%)**
- –ó–Ω–∞–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ {industry}
- –ü–æ–Ω–∏–º–∞–Ω–∏–µ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –æ—Ç—Ä–∞—Å–ª–∏  
- –û–ø—ã—Ç —Ä–µ—à–µ–Ω–∏—è –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á
- –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º

**–ê–î–ê–ü–¢–ò–í–ù–û–°–¢–¨ –ò –†–ê–ó–í–ò–¢–ò–ï (–í–ï–°: 5%)**
- –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é
- –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º
- –õ–∏–¥–µ—Ä—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª

**–≠–¢–ê–ü 5: RISK ASSESSMENT**
- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –∏—Å–ø—ã—Ç–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å—Ä–æ–∫–∞
- –†–∏—Å–∫–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –≤ –∫–æ–º–∞–Ω–¥–µ  
- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —É–¥–µ—Ä–∂–∞–Ω–∏—è
- –°–∫–æ—Ä–æ—Å—Ç—å –≤—ã—Ö–æ–¥–∞ –Ω–∞ –ø–æ–ª–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å

**–°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –ü–†–ê–í–ò–õ–ê –û–¶–ï–ù–ö–ò:**

üî¥ **–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ù–ê–†–£–®–ï–ù–ò–Ø (–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –û–¢–ö–ê–ó):**
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∏–∑ must-have
- –ù–µ–ø—Ä–∞–≤–¥–∏–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ —Ä–µ–∑—é–º–µ
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —É—Ä–æ–≤–Ω—è –ø–æ–∑–∏—Ü–∏–∏

üü° **–°–ï–†–¨–ï–ó–ù–´–ï –ù–ï–î–û–°–¢–ê–¢–ö–ò (–°–ò–õ–¨–ù–û–ï –°–ù–ò–ñ–ï–ù–ò–ï –ë–ê–õ–õ–ê):**
- –°–ª–∞–±—ã–π –æ—Ç—Ä–∞—Å–ª–µ–≤–æ–π –æ–ø—ã—Ç –ø—Ä–∏ –≤—ã—Å–æ–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö
- –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –Ω–∞–≤—ã–∫–∏ –¥–ª—è tech-–ø–æ–∑–∏—Ü–∏–π
- –ß–∞—Å—Ç–∞—è —Å–º–µ–Ω–∞ —Ä–∞–±–æ—Ç—ã –±–µ–∑ –ª–æ–≥–∏—á–Ω–æ–≥–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è

üü¢ **–ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê (–ü–û–í–´–®–ï–ù–ò–ï –ë–ê–õ–õ–ê):**
- –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º –Ω–∞–≤—ã–∫–∞–º
- –£–Ω–∏–∫–∞–ª—å–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞
- –õ–∏–¥–µ—Ä—Å–∫–∏–π –æ–ø—ã—Ç –∏ –Ω–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ

–í–ï–†–ù–ò –°–¢–†–û–ì–û JSON:
{{
    "contact_information": {{
        "email": "–Ω–∞–π–¥–µ–Ω–Ω—ã–π email –∏–ª–∏ null",
        "phone": "–Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω –∏–ª–∏ null", 
        "telegram": "–Ω–∞–π–¥–µ–Ω–Ω—ã–π telegram –∏–ª–∏ null"
    }},
    "industry_analysis": {{
        "detected_industry": "{industry}",
        "industry_specific_requirements": ["—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ 1", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ 2"],
        "critical_success_factors": ["—Ñ–∞–∫—Ç–æ—Ä 1", "—Ñ–∞–∫—Ç–æ—Ä 2"],
        "industry_red_flags_found": ["—Ñ–ª–∞–≥ 1", "—Ñ–ª–∞–≥ 2"] 
    }},
    "vacancy_deep_dive": {{
        "must_have_requirements": ["–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ 1", "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ 2"],
        "nice_to_have_requirements": ["–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–µ 1", "–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–µ 2"],
        "hidden_requirements": ["—Å–∫—Ä—ã—Ç–æ–µ 1", "—Å–∫—Ä—ã—Ç–æ–µ 2"],
        "position_complexity": "junior/middle/senior/expert",
        "team_dynamics_expected": "leadership/collaboration/independent"
    }},
    "candidate_profile": {{
        "total_experience_years": "–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —á–∏—Å–ª–æ –ª–µ—Ç",
        "relevant_experience_years": "–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —á–∏—Å–ª–æ –ª–µ—Ç",
        "expertise_level": "–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–π/—Å—Ä–µ–¥–Ω–∏–π/—ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π",
        "career_trajectory": "—Ä–∞—Å—Ç—É—â–∏–π/—Å—Ç–∞–±–∏–ª—å–Ω—ã–π/—Å–Ω–∏–∂–∞—é—â–∏–π—Å—è",
        "key_achievements": ["–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ 1", "–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ 2"],
        "potential_concerns": ["–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ 1", "–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ 2"]
    }},
    "detailed_scoring": {{
        "mandatory_requirements": {{
            "score": 0-100,
            "weight": 40,
            "critical_violations": ["–Ω–∞—Ä—É—à–µ–Ω–∏–µ 1"],
            "met_requirements": ["–≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ 1", "–≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ 2"],
            "reasoning": "–¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –ø–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º"
        }},
        "professional_expertise": {{
            "score": 0-100,
            "weight": 35,
            "technical_depth": "–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–π/—Å—Ä–µ–¥–Ω–∏–π/—ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π",
            "project_quality": "–Ω–∏–∑–∫–æ–µ/—Å—Ä–µ–¥–Ω–µ–µ/–≤—ã—Å–æ–∫–æ–µ",
            "reasoning": "–æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏"
        }},
        "industry_specialization": {{
            "score": 0-100,
            "weight": 20,
            "domain_match": "—Å–ª–∞–±–æ–µ/—á–∞—Å—Ç–∏—á–Ω–æ–µ/—Ç–æ—á–Ω–æ–µ",
            "transferability_risk": "–≤—ã—Å–æ–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–Ω–∏–∑–∫–∏–π",
            "reasoning": "–∞–Ω–∞–ª–∏–∑ –æ—Ç—Ä–∞—Å–ª–µ–≤–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"
        }},
        "adaptability": {{
            "score": 0-100,
            "weight": 5,
            "learning_ability": "–Ω–∏–∑–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–≤—ã—Å–æ–∫–∞—è",
            "change_management": "—Å–ª–∞–±–æ–µ/—Å—Ä–µ–¥–Ω–µ–µ/—Å–∏–ª—å–Ω–æ–µ",
            "reasoning": "–æ—Ü–µ–Ω–∫–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ —Ä–æ—Å—Ç–∞"
        }}
    }},
    "risk_assessment": {{
        "probation_success_probability": "0-100%",
        "team_integration_risk": "–Ω–∏–∑–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–≤—ã—Å–æ–∫–∏–π",
        "retention_probability": "0-100%",
        "time_to_productivity": "1-3 –º–µ—Å—è—Ü–∞ / 3-6 –º–µ—Å—è—Ü–µ–≤ / 6+ –º–µ—Å—è—Ü–µ–≤",
        "overall_hiring_risk": "–Ω–∏–∑–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–≤—ã—Å–æ–∫–∏–π"
    }},
    "final_evaluation": {{
        "total_score": 0-100,
        "weighted_calculation": "40*(mandatory) + 35*(expertise) + 20*(industry) + 5*(adaptability) = –∏—Ç–æ–≥",
        "recommendation": "strong_hire/hire/conditional_hire/weak_hire/no_hire",
        "confidence_level": "very_high/high/medium/low/very_low",
        "key_strengths": ["—Å–∏–ª—å–Ω–µ–π—à–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ 1", "—Å–∏–ª—å–Ω–µ–π—à–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ 2"],
        "critical_concerns": ["–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞ 1", "–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞ 2"],
        "hiring_rationale": "—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –æ—Ç—Ä–∞—Å–ª–µ–≤–æ–π —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏",
        "onboarding_strategy": "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞",
        "development_plan": "–ø–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è –Ω–∞ –ø–µ—Ä–≤—ã–π –≥–æ–¥",
        "compensation_analysis": "–∞–Ω–∞–ª–∏–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ–∂–∏–¥–∞–Ω–∏–π —Ä—ã–Ω–∫—É"
    }}
}}
"""

    def evaluate_candidate(self, resume_text, vacancy_text, industry):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ GPT-5 —Å –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ—Ç—Ä–∞—Å–ª—å—é"""
        try:
            response = self.client.chat.completions.create(
                model="gpt-5-mini",
                messages=[
                    {"role": "system", "content": "–¢—ã - —ç–ª–∏—Ç–Ω—ã–π HR-—ç–∫—Å–ø–µ—Ä—Ç –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è —Å –≥–ª—É–±–æ–∫–æ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–æ–π –≤ –æ—Ü–µ–Ω–∫–µ —Ç–∞–ª–∞–Ω—Ç–æ–≤."},
                    {"role": "user", "content": self.create_evaluation_prompt(resume_text, vacancy_text, industry)}
                ],
            )
            result_text = response.choices[0].message.content
            
            start = result_text.find('{')
            end = result_text.rfind('}') + 1
            if start != -1 and end != 0:
                json_str = result_text[start:end]
                return json.loads(json_str)
            else:
                return {
                    "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞",
                    "raw_response": result_text
                }
                
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ API: {str(e)}"}

    def create_final_evaluation_prompt(self, vacancy_text, resume_text, interview_log):
        """–ù–û–í–´–ô –ü–†–û–ú–ü–¢ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è"""
        return f"""
–¢—ã - –≥–ª–∞–≤–Ω—ã–π HR-–¥–∏—Ä–µ–∫—Ç–æ—Ä —Å 20-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º, –ø—Ä–∏–Ω–∏–º–∞—é—â–∏–π —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –æ –Ω–∞–π–º–µ.
–¢–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ –∫–∞–Ω–¥–∏–¥–∞—Ç–µ: –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏, –µ–≥–æ —Ä–µ–∑—é–º–µ, –ø–µ—Ä–≤–∏—á–Ω—ã–π AI-–∞–Ω–∞–ª–∏–∑ –∏ –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤—å—é, –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–≥–æ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º.

–¢–í–û–Ø –ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –í–°–ï –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ, –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ.

=== –í–ê–ö–ê–ù–°–ò–Ø ===
{vacancy_text}

=== –†–ï–ó–Æ–ú–ï –ö–ê–ù–î–ò–î–ê–¢–ê ===
{resume_text}

=== –ü–†–û–¢–û–ö–û–õ –¢–ï–•–ù–ò–ß–ï–°–ö–û–ì–û –ò–ù–¢–ï–†–í–¨–Æ ===
{interview_log}

=== –ê–ù–ê–õ–ò–ó ===
–ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑, –æ—Ç–≤–µ—Ç–∏–≤ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:
1.  **–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–µ–∑—é–º–µ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é:** –ü–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏—Å—å –ª–∏ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã, –∑–∞—è–≤–ª–µ–Ω–Ω—ã–µ –≤ —Ä–µ–∑—é–º–µ, –≤ —Ö–æ–¥–µ –∏–Ω—Ç–µ—Ä–≤—å—é? –ë—ã–ª–∏ –ª–∏ –≤—ã—è–≤–ª–µ–Ω—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è?
2.  **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –≥–ª—É–±–∏–Ω–∞:** –ù–∞—Å–∫–æ–ª—å–∫–æ –≥–ª—É–±–æ–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç –ø–æ–Ω–∏–º–∞–µ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –æ –∫–æ—Ç–æ—Ä—ã—Ö –≥–æ–≤–æ—Ä–∏—Ç? –ü—Ä–∏–≤–æ–¥–∏—Ç –ª–∏ –æ–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–ª–∏ –æ—Ç–≤–µ—á–∞–µ—Ç –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ?
3.  **Soft Skills:** –ö–∞–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç –≤–µ–¥–µ—Ç —Å–µ–±—è –≤ –¥–∏–∞–ª–æ–≥–µ? –£–≤–µ—Ä–µ–Ω–Ω–æ? –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ? –°–ø–æ—Å–æ–±–µ–Ω –ª–∏ –ø—Ä–∏–∑–Ω–∞–≤–∞—Ç—å –Ω–µ–∑–Ω–∞–Ω–∏–µ? –ö–∞–∫ —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ —Å–ª–æ–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã?
4.  **–ö–ª—é—á–µ–≤—ã–µ "–∑–µ–ª–µ–Ω—ã–µ —Ñ–ª–∞–≥–∏":** –ö–∞–∫–∏–µ –º–æ–º–µ–Ω—Ç—ã –≤ –∏–Ω—Ç–µ—Ä–≤—å—é –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –≥–æ–≤–æ—Ä—è—Ç –≤ –ø–æ–ª—å–∑—É –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —É—Å–ø–µ—à–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≥–ª—É–±–æ–∫–æ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã)?
5.  **–ö–ª—é—á–µ–≤—ã–µ "–∫—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏":** –ö–∞–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –∏–ª–∏ –º–æ–º–µ–Ω—Ç—ã –≤—ã–∑—ã–≤–∞—é—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–µ –±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ?
6.  **–ò—Ç–æ–≥–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –£—á–∏—Ç—ã–≤–∞—è –≤—Å–µ "–∑–∞" –∏ "–ø—Ä–æ—Ç–∏–≤", –∫–∞–∫–æ–µ —Ç–≤–æ–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ?

=== –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê ===
–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –æ–±—ä–µ–∫—Ç —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
{{
    "evaluation_summary": "–¢–≤–æ–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–±–∑–∞—Ü–∞—Ö, –≥–¥–µ —Ç—ã –æ—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤—Å–µ 6 –≤–æ–ø—Ä–æ—Å–æ–≤ –∞–Ω–∞–ª–∏–∑–∞. –ì–æ–≤–æ—Ä–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –ø–æ–Ω—è—Ç–Ω–æ.",
    "final_recommendation": "–û–î–ù–û –ò–ó: 'hire' (–Ω–∞–Ω—è—Ç—å) –∏–ª–∏ 'no_hire' (–æ—Ç–∫–∞–∑–∞—Ç—å)"
}}
"""

    def get_final_evaluation(self, vacancy_text, resume_text, interview_log):
        """–ü–æ–ª—É—á–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç GPT"""
        try:
            prompt = self.create_final_evaluation_prompt(vacancy_text, resume_text, interview_log)
            response = self.client.chat.completions.create(
                model="gpt-5-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            result_text = response.choices[0].message.content
            return json.loads(result_text)
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ API –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è: {e}"}

def create_score_gauge(score, title):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä—É–≥–æ–≤–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã –æ—Ü–µ–Ω–∫–∏"""
    fig = go.Figure(go.Indicator(
        mode = "gauge+number",
        value = score,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': title, 'font': {'size': 16}},
        gauge = {
            'axis': {'range': [None, 100]},
            'bar': {'color': "darkblue"},
            'steps': [
                {'range': [0, 50], 'color': "lightcoral"},
                {'range': [50, 75], 'color': "gold"}, 
                {'range': [75, 100], 'color': "lightgreen"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 80
            }
        }
    ))
    fig.update_layout(height=250, width=400, margin=dict(l=20, r=20, t=40, b=20))
    return fig

def display_contact_section(contacts, candidate_id, analysis_data, db_manager):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    st.subheader("üìû –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
    
    final_eval = analysis_data.get("final_evaluation", {})
    score = final_eval.get("total_score", 0)
    
    if score >= 50:
        if f"edit_email_{candidate_id}" not in st.session_state:
            st.session_state[f"edit_email_{candidate_id}"] = False
        if f"edit_phone_{candidate_id}" not in st.session_state:
            st.session_state[f"edit_phone_{candidate_id}"] = False
        if f"edit_telegram_{candidate_id}" not in st.session_state:
            st.session_state[f"edit_telegram_{candidate_id}"] = False
        
        col1, col2, col3 = st.columns(3)
        
        contact_info = analysis_data.get("contact_information", {})
        current_email = contacts.get('email') or contact_info.get('email')
        current_phone = contacts.get('phone') or contact_info.get('phone')  
        current_telegram = contacts.get('telegram') or contact_info.get('telegram')
        
        with col1:
            st.write("**Email:**")
            if current_email and not st.session_state[f"edit_email_{candidate_id}"]:
                col_email1, col_email2 = st.columns([4, 1])
                with col_email1:
                    st.success(f"‚úÖ {current_email}")
                with col_email2:
                    if st.button("‚úèÔ∏è", key=f"edit_email_btn_{candidate_id}", help="–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å"):
                        st.session_state[f"edit_email_{candidate_id}"] = True
                        st.rerun()
                new_email = current_email
            else:
                new_email = st.text_input("Email:", value=current_email or "", key=f"email_input_{candidate_id}", placeholder="example@mail.com")
        
        with col2:
            st.write("**–¢–µ–ª–µ—Ñ–æ–Ω:**")
            if current_phone and not st.session_state[f"edit_phone_{candidate_id}"]:
                col_phone1, col_phone2 = st.columns([4, 1])
                with col_phone1:
                    st.success(f"‚úÖ {current_phone}")
                with col_phone2:
                    if st.button("‚úèÔ∏è", key=f"edit_phone_btn_{candidate_id}", help="–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å"):
                        st.session_state[f"edit_phone_{candidate_id}"] = True
                        st.rerun()
                new_phone = current_phone
            else:
                new_phone = st.text_input("–¢–µ–ª–µ—Ñ–æ–Ω:", value=current_phone or "", key=f"phone_input_{candidate_id}", placeholder="+7 999 123-45-67")
        
        with col3:
            st.write("**Telegram:**")
            if current_telegram and not st.session_state[f"edit_telegram_{candidate_id}"]:
                col_tg1, col_tg2 = st.columns([4, 1])
                with col_tg1:
                    st.success(f"‚úÖ {current_telegram}")
                with col_tg2:
                    if st.button("‚úèÔ∏è", key=f"edit_telegram_btn_{candidate_id}", help="–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å"):
                        st.session_state[f"edit_telegram_{candidate_id}"] = True
                        st.rerun()
                new_telegram = current_telegram
            else:
                new_telegram = st.text_input("Telegram:", value=current_telegram or "", key=f"telegram_input_{candidate_id}", placeholder="@username")
        
        if st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç—ã", key=f"save_contacts_{candidate_id}"):
            db_manager.update_candidate_contacts(candidate_id, new_email, new_phone, new_telegram)
            
            st.session_state[f"edit_email_{candidate_id}"] = False
            st.session_state[f"edit_phone_{candidate_id}"] = False
            st.session_state[f"edit_telegram_{candidate_id}"] = False
            
            st.success("–ö–æ–Ω—Ç–∞–∫—Ç—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã!")
            st.rerun()
        
        return {
            'email': new_email,
            'phone': new_phone, 
            'telegram': new_telegram
        }
    
    return None

def display_interview_toggle(analysis_id, current_status, contacts, score, db_manager, analysis):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—è —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è —Å –æ—Ç–ø—Ä–∞–≤–∫–æ–π –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä."""
    st.subheader("üéØ –°—Ç–∞—Ç—É—Å —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è")
    
    if score >= 50:
        
        has_contacts = (contacts and (contacts.get('email') or contacts.get('telegram')))
        
        if not has_contacts:
            st.warning("‚ö†Ô∏è –î–ª—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å Email –∏–ª–∏ Telegram –∫–∞–Ω–¥–∏–¥–∞—Ç–∞")
            return

        col1, col2 = st.columns([3, 1])
        
        with col1:
            current_status_text = "–ù–∞–∑–Ω–∞—á–µ–Ω–æ" if current_status else "–ù–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–æ"
            status_color = "üü¢" if current_status else "üî¥"
            st.write(f"{status_color} **–°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ:** {current_status_text}")
        
        with col2:
            new_status = st.toggle(
                "–ù–∞–∑–Ω–∞—á–∏—Ç—å —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ",
                value=current_status,
                key=f"interview_toggle_{analysis_id}"
            )
        
        if new_status != current_status:
            db_manager.update_interview_status(analysis_id, new_status)
            
            if new_status:
                with st.spinner("–û—Ç–ø—Ä–∞–≤–∫–∞ –∫–æ–º–∞–Ω–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–µ—Ä..."):
                    try:
                        payload = {
                            "webhook_type": "interview_approved",
                            "analysis_id": analysis_id,
                            "contacts": contacts,
                            "vacancy": {
                                "title": analysis['vacancy_title'],
                                "content": analysis['vacancy_content']
                            }
                        }
                        
                        response = requests.post(MAIN_SERVER_URL, json=payload, timeout=10)
                        
                        if response.status_code == 200:
                            st.success("‚úÖ –ö–æ–º–∞–Ω–¥–∞ –Ω–∞ –∑–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–≤—å—é —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞!")
                        else:
                            st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {response.status_code} - {response.text}")
                    
                    except requests.exceptions.RequestException as e:
                        st.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–µ—Ä–≤–µ—Ä—É: {e}")
            
            st.rerun()

def display_results(evaluation, show_full=True):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ (—Ç–æ–ª—å–∫–æ JSON —á–∞—Å—Ç—å)"""
    if "error" in evaluation:
        st.error(f"–û—à–∏–±–∫–∞: {evaluation['error']}")
        if "raw_response" in evaluation:
            with st.expander("–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ—Ç–≤–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"):
                st.text(evaluation["raw_response"])
        return None
    
    russian_names = {
                    'mandatory_requirements': '–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è',
                    'professional_expertise': '–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞', 
                    'industry_specialization': '–û—Ç—Ä–∞—Å–ª–µ–≤–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è',
                    'adaptability': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å'
                    }
    
    final_eval = evaluation.get("final_evaluation", {})
    detailed_scoring = evaluation.get("detailed_scoring", {})
    
    score = final_eval.get("total_score", 0)
    
    if score >= 75:
        recommendation = "strong_hire"
    elif score >= 50:
        recommendation = "hire"
    else:
        recommendation = "no_hire"
    
    confidence = final_eval.get("confidence_level", "medium")
    
    if show_full:
        col1, col2, col3 = st.columns([2, 2, 3])
        
        with col1:
            st.plotly_chart(create_score_gauge(score, f"–û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—é–º–µ: {score}/100"))
        
        with col2:
            rec_map = {
                "strong_hire": ("‚úÖ –ù–ê–°–¢–û–Ø–¢–ï–õ–¨–ù–û –†–ï–ö–û–ú–ï–ù–î–£–Æ", "–ù–ê–ù–Ø–¢–¨ –ù–ï–ú–ï–î–õ–ï–ù–ù–û"),
                "hire": ("‚úÖ –†–ï–ö–û–ú–ï–ù–î–£–Æ", "–ù–ê–ù–Ø–¢–¨"),
                "no_hire": ("‚ùå –ù–ï –†–ï–ö–û–ú–ï–ù–î–£–Æ", "–û–¢–ö–ê–ó–ê–¢–¨")
            }
            
            rec_text, rec_action = rec_map.get(recommendation, ("‚ùì –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–û", "–¢–†–ï–ë–£–ï–¢ –ê–ù–ê–õ–ò–ó"))
            
            if recommendation in ["strong_hire", "hire"]:
                st.success(rec_text)
                st.success(f"**{rec_action}**")
            else:
                st.error(rec_text)
                st.error(f"**{rec_action}**")
                
            confidence_map = {
                "very_high": "–æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è", "high": "–≤—ã—Å–æ–∫–∞—è", "medium": "—Å—Ä–µ–¥–Ω—è—è", 
                "low": "–Ω–∏–∑–∫–∞—è", "very_low": "–æ—á–µ–Ω—å –Ω–∏–∑–∫–∞—è"
            }
            confidence_emoji = {
                "very_high": "üéØ", "high": "üéØ", "medium": "ü§î", "low": "‚ùì", "very_low": "‚ùì"
            }
            conf_ru = confidence_map.get(confidence, "—Å—Ä–µ–¥–Ω—è—è")
            st.info(f"{confidence_emoji.get(confidence, 'ü§î')} –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {conf_ru}")
        
        with col3:
            st.subheader("üí≠ –õ–æ–≥–∏–∫–∞ —Ä–µ—à–µ–Ω–∏—è")
            decision_logic = final_eval.get("hiring_rationale", "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
            st.write(decision_logic)
        
        st.markdown("---")
        
        if "industry_analysis" in evaluation:
            industry_info = evaluation["industry_analysis"]
            st.subheader(f"üè¢ –ê–Ω–∞–ª–∏–∑ –æ—Ç—Ä–∞—Å–ª–∏: {industry_info.get('detected_industry', 'general').title()}")
            col1, col2 = st.columns(2)
            with col1:
                st.write("**–°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –æ—Ç—Ä–∞—Å–ª–∏:**")
                for req in industry_info.get("industry_specific_requirements", []):
                    st.write(f"‚Ä¢ {req}")
            with col2:
                if industry_info.get("industry_red_flags_found"):
                    st.write("**–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∫—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏:**")
                    for flag in industry_info["industry_red_flags_found"]:
                        st.write(f"üö© {flag}")
            st.markdown("---")
        
        st.subheader("üìä –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º")
        if detailed_scoring:
            scores_data = []
            for criterion, data in detailed_scoring.items():
                if isinstance(data, dict) and "score" in data:
                    weight = data.get('weight', 0)
                    
                    criterion_name = russian_names.get(criterion, criterion.replace('_', ' ').title())
                    scores_data.append({
                        "–ö—Ä–∏—Ç–µ—Ä–∏–π": f"{criterion_name}\n({weight}%)",
                        "–û—Ü–µ–Ω–∫–∞": data["score"],
                        "–í–µ—Å": weight
                    })
            if scores_data:
                fig = px.bar(scores_data, x="–ö—Ä–∏—Ç–µ—Ä–∏–π", y="–û—Ü–µ–Ω–∫–∞", title="–û—Ü–µ–Ω–∫–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º",
                             color="–û—Ü–µ–Ω–∫–∞", color_continuous_scale="RdYlGn", range_color=[0, 100])
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)
        
        if "risk_assessment" in evaluation:
            risk_info = evaluation["risk_assessment"]
            st.subheader("‚ö†Ô∏è –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ –Ω–∞–π–º–∞")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("–£—Å–ø–µ—Ö –∏—Å–ø—ã—Ç–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å—Ä–æ–∫–∞", risk_info.get("probation_success_probability", "N/A"))
                st.metric("–£–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞", risk_info.get("retention_probability", "N/A"))
            with col2:
                st.metric("–†–∏—Å–∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –∫–æ–º–∞–Ω–¥—É", risk_info.get("team_integration_risk", "—Å—Ä–µ–¥–Ω–∏–π"))
                st.metric("–í—Ä–µ–º—è –≤—ã—Ö–æ–¥–∞ –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", risk_info.get("time_to_productivity", "3-6 –º–µ—Å—è—Ü–µ–≤"))
            with col3:
                overall_risk = risk_info.get("overall_hiring_risk", "—Å—Ä–µ–¥–Ω–∏–π")
                if overall_risk == "–Ω–∏–∑–∫–∏–π": st.success(f"–û–±—â–∏–π —Ä–∏—Å–∫ –Ω–∞–π–º–∞: {overall_risk}")
                elif overall_risk == "—Å—Ä–µ–¥–Ω–∏–π": st.warning(f"–û–±—â–∏–π —Ä–∏—Å–∫ –Ω–∞–π–º–∞: {overall_risk}")
                else: st.error(f"–û–±—â–∏–π —Ä–∏—Å–∫ –Ω–∞–π–º–∞: {overall_risk}")
        
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("üí™ –ö–ª—é—á–µ–≤—ã–µ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã")
            strengths = final_eval.get("key_strengths", [])
            for strength in strengths: st.write(f"‚Ä¢ {strength}")
            if not strengths: st.write("–ù–µ –≤—ã—è–≤–ª–µ–Ω—ã")
        with col2:
            st.subheader("‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ—á–∞–Ω–∏—è")
            concerns = final_eval.get("critical_concerns", [])
            for concern in concerns: st.write(f"‚Ä¢ {concern}")
            if not concerns: st.write("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
        
        if "development_plan" in final_eval:
            st.subheader("üìà –ü–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞")
            st.info(final_eval["development_plan"])
        
        st.subheader("üîç –ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º")
        for criterion, data in detailed_scoring.items():
            if isinstance(data, dict):
                score = data.get('score', 0)
                reasoning = data.get('reasoning', '–ê–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω')
                if score >= 80: color = "üü¢"
                elif score >= 60: color = "üü°"
                else: color = "üî¥"
                criterion_name = russian_names.get(criterion, criterion.replace('_', ' ').title())
                with st.expander(f"{color} {criterion_name} - {score}/100"):
                    st.write(reasoning)
    
    return {
        "score": score,
        "recommendation": recommendation,
        "confidence": confidence
    }

def show_history_sidebar(db_manager):
    """–ë–æ–∫–æ–≤–æ–µ –º–µ–Ω—é —Å –∫–Ω–æ–ø–∫–æ–π –≤–æ–∑–≤—Ä–∞—Ç–∞ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏ –∏–Ω—Ç–µ—Ä–≤—å—é"""
    
    if st.sidebar.button("üè† –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –∞–Ω–∞–ª–∏–∑—É", use_container_width=True):
        st.session_state.current_view = 'main'
        st.session_state.selected_analysis_id = None
        st.rerun()
    
    st.sidebar.markdown("---")
    st.sidebar.header("üìö –ò—Å—Ç–æ—Ä–∏—è –∞–Ω–∞–ª–∏–∑–æ–≤")
    
    search_term = st.sidebar.text_input("üîç –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–∏–∑–æ–≤", placeholder="–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç...")
    
    if search_term:
        analyses = db_manager.search_analyses(search_term)
    else:
        analyses = db_manager.get_all_analyses()
    
    if not analyses:
        st.sidebar.info("–ê–Ω–∞–ª–∏–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return None
    
    total_analyses = len(analyses)
    
    hired_count = sum(1 for a in analyses if (a['total_score'] or 0) >= 50)
    
    avg_score = sum(a['total_score'] or 0 for a in analyses) / total_analyses if total_analyses > 0 else 0
    
    st.sidebar.metric("–í—Å–µ–≥–æ –∞–Ω–∞–ª–∏–∑–æ–≤", total_analyses)
    st.sidebar.metric("–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ã –∫ –Ω–∞–π–º—É", hired_count)  
    st.sidebar.metric("–°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª", f"{avg_score:.1f}")
    
    st.sidebar.markdown("---")
    
    selected_analysis = None
    
    for analysis in analyses[:20]:
        created_at = datetime.fromisoformat(analysis['created_at'].replace('Z', '+00:00')).strftime("%d.%m.%Y %H:%M")
        
        if (analysis['total_score'] or 0) >= 50:
            emoji = "‚úÖ"
        else:
            emoji = "‚ùå"
        
        interview_emoji = "üéôÔ∏è" if analysis['interview_id'] else ""
        
        final_eval_emoji = "üèÜ" if 'final_evaluation_id' in analysis.keys() and analysis['final_evaluation_id'] else ""
        
        button_text = f"{emoji}{interview_emoji}{final_eval_emoji} {analysis['candidate_name'][:15]}...\n{analysis['vacancy_title'][:20]}...\n{created_at}"
        
        if st.sidebar.button(button_text, key=f"analysis_{analysis['id']}", help=f"–ë–∞–ª–ª: {analysis['total_score']}/100"):
            selected_analysis = analysis['id']
    
    return selected_analysis

def display_interview_report(analysis_data, interview_data, qa_data, db_manager, evaluator):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—é"""
    try:
        final_scores = json.loads(interview_data.get('final_scores', '{}'))
        phase_breakdown = json.loads(interview_data.get('phase_breakdown', '{}'))
        
        overall_score = final_scores.get('overall_score', 0)
        
        resume_score = analysis_data['total_score'] if 'total_score' in analysis_data.keys() else 0

        if overall_score >= 50 and resume_score >= 50:
            recommendation_text = "hire"
        else:
            recommendation_text = "no_hire"
        
        col1, col2, col3 = st.columns([2, 1, 2])
        with col1:
            st.plotly_chart(create_score_gauge(overall_score, "–û—Ü–µ–Ω–∫–∞ –∑–∞ –∏–Ω—Ç–µ—Ä–≤—å—é"))
        with col2:
            st.metric("–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", f"{interview_data.get('duration_seconds', 0) // 60} –º–∏–Ω")
            st.metric("–ó–∞–¥–∞–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤", interview_data.get('total_questions', 0))
        with col3:
            rec_map = {
                "hire": ("‚úÖ –ö–ê–ù–î–ò–î–ê–¢ –ü–û–î–•–û–î–ò–¢", "success"),
                "no_hire": ("‚ùå –ö–ê–ù–î–ò–î–ê–¢ –ù–ï –ü–û–î–•–û–î–ò–¢", "error")
            }
            rec_text, rec_type = rec_map.get(recommendation_text, ("‚ùì –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ", "info"))
            
            if rec_type == "success":
                st.success(rec_text)
                st.info(f"–û—Ü–µ–Ω–∫–∞ –∑–∞ —Ä–µ–∑—é–º–µ: {resume_score}/100\n–û—Ü–µ–Ω–∫–∞ –∑–∞ –∏–Ω—Ç–µ—Ä–≤—å—é: {overall_score}/100")
            else:
                st.error(rec_text)
                st.warning(f"–û—Ü–µ–Ω–∫–∞ –∑–∞ —Ä–µ–∑—é–º–µ: {resume_score}/100\n–û—Ü–µ–Ω–∫–∞ –∑–∞ –∏–Ω—Ç–µ—Ä–≤—å—é: {overall_score}/100")

        st.markdown("---")

        st.subheader("üèÜ –§–∏–Ω–∞–ª—å–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –ø–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—É")
        
        final_evaluation = db_manager.get_final_evaluation(analysis_data['id'])

        if final_evaluation:
            st.success("–ó–∞–∫–ª—é—á–µ–Ω–∏–µ –±—ã–ª–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ä–∞–Ω–µ–µ:")
            st.markdown(final_evaluation['evaluation_summary'])
            if final_evaluation['final_recommendation'] == 'hire':
                st.success("–ò—Ç–æ–≥–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: **–ù–∞–Ω—è—Ç—å**")
            else:
                st.error("–ò—Ç–æ–≥–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: **–û—Ç–∫–∞–∑–∞—Ç—å**")
        
        if st.button("ü§ñ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å/–æ–±–Ω–æ–≤–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ", key="generate_final_eval"):
            with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –≤—Å–µ –¥–∞–Ω–Ω—ã–µ... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–∏–Ω—É—Ç—É..."):
                vacancy_text = analysis_data['vacancy_content']
                resume_text = analysis_data['candidate_resume']
                
                interview_log_parts = []
                for qa in qa_data:
                    interview_log_parts.append(f"–í–æ–ø—Ä–æ—Å {qa['question_number']}: {qa['question_text']}")
                    interview_log_parts.append(f"–û—Ç–≤–µ—Ç: {qa['answer_text']}")
                    analysis_notes = qa['analysis_notes'] if 'analysis_notes' in qa.keys() and qa['analysis_notes'] else 'N/A'
                    interview_log_parts.append(f"–ê–Ω–∞–ª–∏–∑ AI: {analysis_notes}\n")
                interview_log = "\n".join(interview_log_parts)

                result = evaluator.get_final_evaluation(vacancy_text, resume_text, interview_log)
                
                if "error" not in result:
                    summary = result.get("evaluation_summary", "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞–∫–ª—é—á–µ–Ω–∏–µ.")
                    recommendation = result.get("final_recommendation", "no_hire")
                    db_manager.save_final_evaluation(analysis_data['id'], summary, recommendation)
                    st.success("–§–∏–Ω–∞–ª—å–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ!")
                    st.rerun()
                else:
                    st.error(result["error"])

        st.markdown("---")

        st.subheader("üìà –ê–Ω–∞–ª–∏–∑ –ø–æ —Ñ–∞–∑–∞–º –∏–Ω—Ç–µ—Ä–≤—å—é")
        if phase_breakdown:
            phase_data = []
            phase_names = {
                'exploration': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ',
                'validation': '–ü—Ä–æ–≤–µ—Ä–∫–∞', 
                'stress_test': '–°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç',
                'soft_skills': '–°–æ—Ñ—Ç —Å–∫–∏–ª—ã',
                'wrap_up': '–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ'
            }
            for phase, stats in phase_breakdown.items():
                phase_data.append({
                "–§–∞–∑–∞": phase_names.get(phase, phase.replace('_', ' ').title()),
                "–°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª": stats.get('avg_score', 0),
                "–í–æ–ø—Ä–æ—Å–æ–≤": stats.get('questions_asked', 0)
            })
            
            df = pd.DataFrame(phase_data)
            fig = px.bar(df, x="–§–∞–∑–∞", y="–°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª", text="–í–æ–ø—Ä–æ—Å–æ–≤",
                         title="–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —Ñ–∞–∑–∞–º –∏–Ω—Ç–µ—Ä–≤—å—é",
                         color="–°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª", color_continuous_scale="RdYlGn",
                         range_color=[0, 10])
            fig.update_traces(texttemplate='%{text} –≤–æ–ø—Ä.', textposition='outside')
            st.plotly_chart(fig, use_container_width=True)

        st.markdown("---")

        with st.expander("üìù –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è (Q&A)"):
            if not qa_data:
                st.info("–î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.")
            for qa in qa_data:
                st.markdown(f"**–í–æ–ø—Ä–æ—Å {qa['question_number']} (–§–∞–∑–∞: {qa['question_phase']}, –°–ª–æ–∂–Ω–æ—Å—Ç—å: {qa['question_difficulty']}):**")
                st.markdown(f"> {qa['question_text']}")
                st.markdown("**–û—Ç–≤–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:**")
                st.text_area("", value=qa['answer_text'], height=100, disabled=True, key=f"ans_{qa['id']}")
                
                st.markdown("**–ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞:**")
                analysis_cols = st.columns(4)
                analysis_cols[0].metric("–¢–µ—Ö. –Ω–∞–≤—ã–∫–∏", f"{qa['technical_score']}/10")
                analysis_cols[1].metric("–ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è", f"{qa['communication_score']}/10")
                analysis_cols[2].metric("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", f"{qa['confidence_score']}/10")
                analysis_cols[3].metric("–ì–ª—É–±–∏–Ω–∞", f"{qa['depth_score']}/10")
                
                analysis_notes = qa['analysis_notes'] if 'analysis_notes' in qa.keys() and qa['analysis_notes'] else None
                if analysis_notes:
                    st.info(f"**–ó–∞–º–µ—Ç–∫–∏ –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞:** {analysis_notes}")
                st.markdown("---")

    except (json.JSONDecodeError, KeyError) as e:
        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å –æ—Ç—á–µ—Ç –ø–æ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—é. –û—à–∏–±–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")

def show_analysis_details(db_manager, analysis_id):
    """–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –≤–∫–ª–∞–¥–∫–æ–π –¥–ª—è –∏–Ω—Ç–µ—Ä–≤—å—é"""
    analysis = db_manager.get_analysis_by_id(analysis_id)
    evaluator = CandidateEvaluator()
    
    if not analysis:
        st.error("–ê–Ω–∞–ª–∏–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    st.header(f"üìÑ –ê–Ω–∞–ª–∏–∑: {analysis['candidate_name']}")
    st.subheader(f"–í–∞–∫–∞–Ω—Å–∏—è: {analysis['vacancy_title']}")
    
    created_at = datetime.fromisoformat(analysis['created_at'].replace('Z', '+00:00')).strftime("%d.%m.%Y –≤ %H:%M")
    st.caption(f"–°–æ–∑–¥–∞–Ω: {created_at}")
    
    st.markdown("---")
    
    interview_data, qa_data = db_manager.get_interview_by_analysis_id(analysis_id)
    
    if interview_data:
        tab1, tab2 = st.tabs(["üìÑ –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ", "üéôÔ∏è –û—Ç—á–µ—Ç –ø–æ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—é"])
        
        with tab1:
            display_resume_analysis(analysis, db_manager)
            
        with tab2:
            display_interview_report(analysis, interview_data, qa_data, db_manager, evaluator)
            
    else:
        display_resume_analysis(analysis, db_manager)

def display_resume_analysis(analysis, db_manager):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–∞—Å—Ç–∏, –∫–∞—Å–∞—é—â–µ–π—Å—è —Ç–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ"""
    try:
        result_json = json.loads(analysis['result_json'])
        
        display_results(result_json, show_full=True)
        
        final_eval = result_json.get("final_evaluation", {})
        score = final_eval.get("total_score", 0)

        if score >= 50:
            st.markdown("---")
            
            contacts = {
                'email': analysis['candidate_email'],
                'phone': analysis['candidate_phone'],
                'telegram': analysis['candidate_telegram']
            }
            candidate_id = analysis['candidate_id']
            
            updated_contacts = display_contact_section(contacts, candidate_id, result_json, db_manager)
            
            final_contacts = updated_contacts or contacts
            current_interview_status = analysis['needs_interview']
            
            display_interview_toggle(analysis['id'], current_interview_status, final_contacts, score, db_manager, dict(analysis))
        
    except json.JSONDecodeError:
        st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞")
    
    with st.expander("üìã –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"):
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("–û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏")
            st.text_area("", value=analysis['vacancy_content'], height=300, disabled=True, key="vacancy_content")
        with col2:
            st.subheader("–†–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞")  
            st.text_area("", value=analysis['candidate_resume'], height=300, disabled=True, key="candidate_resume")

def show_multiple_resumes_interface(db_manager, evaluator):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ"""
    st.header("üë• –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏")
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    col1, col2 = st.columns([1, 2], gap="large")
    
    with col1:
        st.subheader("üìã –í–∞–∫–∞–Ω—Å–∏—è")
        st.markdown("<br>", unsafe_allow_html=True)
        
        vacancy_file = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏",
            type=['docx', 'pdf', 'txt', 'rtf'],
            key="multi_vacancy"
        )
        
        st.markdown("<br>", unsafe_allow_html=True)
        
        vacancy_text = st.text_area(
            "–ò–ª–∏ –≤–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ:",
            height=200,
            key="multi_vacancy_text"
        )
        
        st.markdown("<br>", unsafe_allow_html=True)
        
        vacancy_title = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏:", key="multi_vacancy_title")
    
    with col2:
        st.subheader("üë§ –†–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        st.markdown("<br>", unsafe_allow_html=True)
        
        resume_files = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ (–º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤)",
            type=['docx', 'pdf', 'txt', 'rtf'],
            accept_multiple_files=True,
            key="multi_resumes"
        )
        
        if resume_files:
            st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ä–µ–∑—é–º–µ: {len(resume_files)} —Ñ–∞–π–ª–æ–≤")
            for i, file in enumerate(resume_files):
                st.write(f"{i+1}. {file.name}")
    
    st.markdown("<br><br>", unsafe_allow_html=True)
    
    if st.button("üöÄ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤", type="primary", use_container_width=True):
        if not (vacancy_file or vacancy_text.strip()):
            st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏")
            return
            
        if not resume_files:
            st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
            return
        
        if vacancy_file:
            vacancy_content = evaluator.extract_text_from_file(vacancy_file)
            v_title = vacancy_title or vacancy_file.name.split('.')[0]
        else:
            vacancy_content = vacancy_text.strip()
            v_title = vacancy_title or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        
        if vacancy_content.startswith("–û–®–ò–ë–ö–ê"):
            st.error(vacancy_content)
            return
        
        with st.spinner("üîç –û–ø—Ä–µ–¥–µ–ª—è—é –æ—Ç—Ä–∞—Å–ª—å –≤–∞–∫–∞–Ω—Å–∏–∏..."):
            industry = evaluator.classify_industry(vacancy_content)
            st.success(f"‚úÖ –û—Ç—Ä–∞—Å–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: {industry}")
        
        results = []
        vacancy_id = db_manager.save_vacancy(v_title, vacancy_content, industry)
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, resume_file in enumerate(resume_files):
            status_text.text(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è: {resume_file.name} ({i+1}/{len(resume_files)})")
            
            resume_content = evaluator.extract_text_from_file(resume_file)
            
            if resume_content.startswith("–û–®–ò–ë–ö–ê"):
                results.append({
                    'candidate_name': resume_file.name,
                    'error': resume_content,
                    'score': 0,
                    'recommendation': 'error'
                })
                continue
            
            evaluation = evaluator.evaluate_candidate(resume_content, vacancy_content, industry)
            
            if evaluation and "error" not in evaluation:
                final_eval = evaluation.get("final_evaluation", {})
                candidate_name = resume_file.name.split('.')[0]
                
                contact_info = evaluation.get("contact_information", {})
                
                score = final_eval.get("total_score", 0)
                needs_interview = score >= 50 and (contact_info.get('email') or contact_info.get('telegram'))
                
                candidate_id = db_manager.save_candidate(
                    candidate_name, 
                    resume_content,
                    contact_info.get('email'),
                    contact_info.get('phone'),
                    contact_info.get('telegram')
                )
                
                analysis_id = db_manager.save_analysis(
                    vacancy_id, 
                    candidate_id, 
                    evaluation,
                    score,
                    "hire" if score >= 50 else "no_hire",
                    final_eval.get("confidence_level", "medium"),
                    "batch_multiple_resumes",
                    needs_interview
                )
                
                results.append({
                    'candidate_name': candidate_name,
                    'analysis_id': analysis_id,
                    'score': score,
                    'recommendation': "hire" if score >= 50 else "no_hire",
                    'confidence': final_eval.get("confidence_level", "medium"),
                    'key_strengths': final_eval.get("key_strengths", []),
                    'critical_concerns': final_eval.get("critical_concerns", [])
                })
            else:
                results.append({
                    'candidate_name': resume_file.name,
                    'error': evaluation.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'),
                    'score': 0,
                    'recommendation': 'error'
                })
            
            progress_bar.progress((i + 1) / len(resume_files))
        
        status_text.text("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        
        st.subheader("üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        
        results_sorted = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
        
        table_data = []
        for i, result in enumerate(results_sorted):
            if 'error' in result:
                table_data.append({
                    '–ú–µ—Å—Ç–æ': f"{i+1}",
                    '–ö–∞–Ω–¥–∏–¥–∞—Ç': result['candidate_name'],
                    '–ë–∞–ª–ª': "‚ùå –û—à–∏–±–∫–∞",
                    '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è': "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
                    '–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å': "-"
                })
            else:
                rec_emoji = {
                    'hire': '‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É—é',
                    'no_hire': '‚ùå –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é'
                }
                
                conf_emoji = {
                    'very_high': 'üéØ', 'high': 'üéØ', 'medium': 'ü§î', 'low': '‚ùì', 'very_low': '‚ùì'
                }
                
                table_data.append({
                    '–ú–µ—Å—Ç–æ': f"ü•á {i+1}" if i == 0 else f"ü•à {i+1}" if i == 1 else f"ü•â {i+1}" if i == 2 else f"{i+1}",
                    '–ö–∞–Ω–¥–∏–¥–∞—Ç': result['candidate_name'],
                    '–ë–∞–ª–ª': f"{result['score']}/100",
                    '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è': rec_emoji.get(result['recommendation'], result['recommendation']),
                    '–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å': f"{conf_emoji.get(result['confidence'], 'ü§î')} {result['confidence']}"
                })
        
        df = pd.DataFrame(table_data)
        st.dataframe(df, use_container_width=True)
        
        st.subheader("üèÜ –¢–æ–ø-3 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞")
        
        for i, result in enumerate(results_sorted[:3]):
            if 'error' not in result:
                with st.expander(f"{'ü•á' if i==0 else 'ü•à' if i==1 else 'ü•â'} {result['candidate_name']} - {result['score']}/100"):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:**")
                        for strength in result.get('key_strengths', []):
                            st.write(f"‚Ä¢ {strength}")
                    
                    with col2:
                        st.write("**–ó–∞–º–µ—á–∞–Ω–∏—è:**")
                        for concern in result.get('critical_concerns', []):
                            st.write(f"‚Ä¢ {concern}")
                    
                    if st.button(f"–ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ {result['candidate_name']}", key=f"detail_{result['analysis_id']}"):
                        st.session_state.current_view = 'analysis_detail'
                        st.session_state.selected_analysis_id = result['analysis_id']
                        st.rerun()
        
        db_manager.save_batch_analysis(
            f"Batch –∞–Ω–∞–ª–∏–∑: {v_title}",
            "multiple_resumes",
            results
        )

def main():
    db_type = os.getenv('DATABASE_TYPE', 'sqlite')
    db_manager = DatabaseManager(db_type)
    evaluator = CandidateEvaluator()
    
    st.title("HR-–ê–≤–∞—Ç–∞—Ä: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
    st.markdown("*–£–º–Ω—ã–π –ø–æ–¥–±–æ—Ä –ø–µ—Ä—Å–æ–Ω–∞–ª–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò*")
    st.markdown("---")
    
    if st.session_state.current_view == 'analysis_detail':
        selected_analysis_id = show_history_sidebar(db_manager)
        
        if selected_analysis_id:
            st.session_state.selected_analysis_id = selected_analysis_id
            st.rerun()
        
        if st.session_state.selected_analysis_id:
            show_analysis_details(db_manager, st.session_state.selected_analysis_id)
        
        return
    
    selected_analysis_id = show_history_sidebar(db_manager)
    
    if selected_analysis_id:
        st.session_state.current_view = 'analysis_detail'
        st.session_state.selected_analysis_id = selected_analysis_id
        st.rerun()
    
    st.header("üÜï –ù–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑")
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    analysis_mode = st.selectbox(
        "**–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞:**",
        options=[
            "single",
            "multiple_resumes"
        ],
        format_func=lambda x: {
            "single": "üìÑ –û–¥–Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—è ‚Üí –û–¥–Ω–æ —Ä–µ–∑—é–º–µ",
            "multiple_resumes": "üìÑ –û–¥–Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—è ‚Üí –ù–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ–∑—é–º–µ"
        }[x],
        key="analysis_mode_select"
    )
    
    st.session_state.analysis_mode = analysis_mode
    
    st.markdown("<br><br>", unsafe_allow_html=True)
    
    if analysis_mode == "multiple_resumes":
        show_multiple_resumes_interface(db_manager, evaluator)
        
    else:
        st.header("üìÑ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        
        st.markdown("<br>", unsafe_allow_html=True)
        
        col1, col2 = st.columns(2, gap="large")
        
        with col1:
            st.subheader("üìã –û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏")
            st.markdown("<br>", unsafe_allow_html=True)
            
            vacancy_file = st.file_uploader(
                "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏",
                type=['docx', 'pdf', 'txt', 'rtf'],
                key="single_vacancy"
            )
            
            st.markdown("<br>", unsafe_allow_html=True)
            
            vacancy_text = st.text_area(
                "–ò–ª–∏ –≤–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤—Ä—É—á–Ω—É—é:",
                height=200,
                key="single_vacancy_text",
                placeholder="–í—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å—é–¥–∞..."
            )
            
            st.markdown("<br>", unsafe_allow_html=True)
            
            vacancy_title = st.text_input(
                "–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏:",
                placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: Senior Python Developer"
            )
        
        with col2:
            st.subheader("üë§ –†–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞")
            st.markdown("<br>", unsafe_allow_html=True)
            
            resume_file = st.file_uploader(
                "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞",
                type=['docx', 'pdf', 'txt', 'rtf'],
                key="single_resume"
            )
            
            st.markdown("<br>", unsafe_allow_html=True)
            
            resume_text = st.text_area(
                "–ò–ª–∏ –≤–≤–µ–¥–∏—Ç–µ —Ä–µ–∑—é–º–µ –≤—Ä—É—á–Ω—É—é:",
                height=200,
                key="single_resume_text",
                placeholder="–í—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ —Å—é–¥–∞..."
            )
            
            st.markdown("<br>", unsafe_allow_html=True)
            
            candidate_name = st.text_input(
                "–ò–º—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:",
                placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤"
            )
        
        st.markdown("<br><br>", unsafe_allow_html=True)
        
        if st.button("üöÄ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–∞", type="primary", use_container_width=True):
            final_vacancy_text = ""
            final_resume_text = ""
            
            final_vacancy_title = vacancy_title
            final_candidate_name = candidate_name
            
            if vacancy_file:
                with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª –≤–∞–∫–∞–Ω—Å–∏–∏..."):
                    final_vacancy_text = evaluator.extract_text_from_file(vacancy_file)
                    
                    if final_vacancy_text.startswith("–û–®–ò–ë–ö–ê"):
                        st.error(final_vacancy_text)
                        st.stop()
                    else:
                        st.success(f"‚úÖ –í–∞–∫–∞–Ω—Å–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {len(final_vacancy_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                        
                        if not final_vacancy_title:
                            final_vacancy_title = vacancy_file.name.split('.')[0]
                            
            elif vacancy_text.strip():
                final_vacancy_text = vacancy_text.strip()
                st.success("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏, –≤–≤–µ–¥–µ–Ω–Ω—ã–π –≤—Ä—É—á–Ω—É—é")
                if not final_vacancy_title:
                    final_vacancy_title = "–í–∞–∫–∞–Ω—Å–∏—è –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            
            if resume_file:
                with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ä–µ–∑—é–º–µ..."):
                    final_resume_text = evaluator.extract_text_from_file(resume_file)
                    
                    if final_resume_text.startswith("–û–®–ò–ë–ö–ê"):
                        st.error(final_resume_text)
                        st.stop()
                    else:
                        st.success(f"‚úÖ –†–µ–∑—é–º–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(final_resume_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                        
                        if not final_candidate_name:
                            final_candidate_name = resume_file.name.split('.')[0]
                            
            elif resume_text.strip():
                final_resume_text = resume_text.strip()
                st.success("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ, –≤–≤–µ–¥–µ–Ω–Ω—ã–π –≤—Ä—É—á–Ω—É—é")
                if not final_candidate_name:
                    final_candidate_name = "–ö–∞–Ω–¥–∏–¥–∞—Ç –±–µ–∑ –∏–º–µ–Ω–∏"
            
            if not final_vacancy_text:
                st.error("‚ùå –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–ª–∏ –≤–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤—Ä—É—á–Ω—É—é")
                st.stop()
                
            if not final_resume_text:
                st.error("‚ùå –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ –∏–ª–∏ –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É—é") 
                st.stop()
            
            with st.spinner("üîç –û–ø—Ä–µ–¥–µ–ª—è—é –æ—Ç—Ä–∞—Å–ª—å –≤–∞–∫–∞–Ω—Å–∏–∏..."):
                industry = evaluator.classify_industry(final_vacancy_text)
                st.success(f"‚úÖ –û—Ç—Ä–∞—Å–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: {industry}")
            
            with st.spinner("üß† –ü—Ä–æ–≤–æ–¥–∏—Ç—Å—è –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞... –≠—Ç–æ –∑–∞–π–º–µ—Ç –ø–∞—Ä—É –º–∏–Ω—É—Ç."):
                evaluation = evaluator.evaluate_candidate(final_resume_text, final_vacancy_text, industry)
                
                if evaluation and "error" not in evaluation:
                    st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
                    st.markdown("---")
                    
                    try:
                        vacancy_id = db_manager.save_vacancy(
                            final_vacancy_title,
                            final_vacancy_text,
                            industry
                        )
                        
                        contact_info = evaluation.get("contact_information", {})
                        
                        candidate_id = db_manager.save_candidate(
                            final_candidate_name,
                            final_resume_text,
                            contact_info.get('email'),
                            contact_info.get('phone'),
                            contact_info.get('telegram')
                        )
                        
                        final_eval = evaluation.get("final_evaluation", {})
                        score = final_eval.get("total_score", 0)
                        recommendation = "hire" if score >= 50 else "no_hire"
                        needs_interview = score >= 50 and (contact_info.get('email') or contact_info.get('telegram'))
                        
                        analysis_id = db_manager.save_analysis(
                            vacancy_id,
                            candidate_id,
                            evaluation,
                            score,
                            recommendation,
                            final_eval.get("confidence_level", "medium"),
                            "single",
                            needs_interview
                        )
                        
                        contacts = {
                            'email': contact_info.get('email'),
                            'phone': contact_info.get('phone'),
                            'telegram': contact_info.get('telegram')
                        }
                        
                        display_results(evaluation)

                        final_eval = evaluation.get("final_evaluation", {})
                        score = final_eval.get("total_score", 0)

                        if score >= 50:
                            st.markdown("---")
                            
                            contacts = {
                                'email': contact_info.get('email'),
                                'phone': contact_info.get('phone'),
                                'telegram': contact_info.get('telegram')
                            }
                            
                            updated_contacts = display_contact_section(contacts, candidate_id, evaluation, db_manager)
                            
                            final_contacts = updated_contacts or contacts
                            
                            analysis_for_toggle = {
                                'id': analysis_id,
                                'vacancy_title': final_vacancy_title,
                                'vacancy_content': final_vacancy_text
                            }
                            display_interview_toggle(analysis_id, needs_interview, final_contacts, score, db_manager, analysis_for_toggle)
                        
                        st.info(f"üíæ –ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (ID: {analysis_id[:8]}...)")
                        
                    except Exception as e:
                        st.warning(f"–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î: {e}")
                        display_results(evaluation)

if __name__ == "__main__":
    main()